---
layout: post
title: A Better Taxonomy for Software Tests
date: 2024-01-27
author: Matthew Russell
---

A frequent refrain in software engineering is to test, test, test your code. While easier said than done, it *is* incredibly satisfying to have an extensive suite of tests that verifies your code works as intended. Plus it gives you confidence when you make future changes&mdash;if¬†the tests continue to pass, then you probably haven't broken anything. But *good* testing turns out to be hard.

## Testing code is conceptually simple

Here's a test at it's most basic level:

```python
# ---- greeting.py ---- #
def make_greeting(name: str) -> str:
  """Create a greeting message.
  
  Args:	
  	name: The name of the person to greet.
  	
  Returns:
  	The greeting message.
  """
  return f"Hello, {name}!"


# ---- test_greeting.py ---- #
def test_make_greeting():
  expected = "Hello, Emma!"
  actual = make_greeting("Emma")
  assert actual == expected
```

We define a function `make_greeting` with a single job: return a greeting for a name. Testing this is straightforward&mdash;call the function with a name and check that the result matches what we expect. Take this to the extreme and we end up with the very-nice-in-theory *Test-Driven Development* or TDD. With TDD, we *start* by writing the test and *then* the actual code.[^1] In a sense, the test describes what the code should do and does so in a way that can actually tell us if the code is up to snuff. The test is like writing a contract for what the code should do. Once the test passes, we know our code achieves the goal. We might start with this empty function and corresponding test:

```python
def replace_bear_with_emoji(message: str) -> str:
  pass

def test_replace_bear_with_emoji():
  cases = [
    ("I like bears.", "I like üêªs"),
    ("There once was a bear named Bruno.", "There once was a üêª named Bruno.")
  ]
  for inputs, expected in cases:
    out = replace_bear_with_emoji(inputs)
    assert out == expected
```

This test will fail since the function is empty. However, it will pass once we fill it in like this:

```python
def replace_bear_with_emoji(message: str) -> str:
  return message.replace("bear", "üêª")
```

And that's TDD, more or less. Conceptually, all this is (fairly) simple. 

## Testing *actual* code is very confusing

However, testing code in the wild quickly becomes murky. Writing a test that checks a condition is easy, but how do you know it's a really a *good* test? To answer this, we need a mental model of what we're doing at a more abstract level. We have this tool but we also need a set of guidelines to create truly meaningful tests. We're given a hammer and some nails, told to go build something, but nobody really tells us what the "something" should look like.

This is at least partly because there are lots of different things you *could* build. Unfortunately, I've found conversations around software testing often hit with a not-so-helpful one-two punch:

1. They assume a particular type of testing&mdash;the "something" we're supposed to build&mdash;but don't make that assumption clear.
2. They make absolute statements: ALWAYS do this, NEVER do that.

We end up with a bunch of frustrating hard-and-fast rules for how to write tests that might not even apply to our use case.  

Take unit testing for example. It's easy to feel like unit testing is the only "right" way to test&mdash;testing in it's purest form. We take each piece of code in complete isolation (like the simple example from the previous section) and run it through a series of comprehensive cases that cover both "happy paths" and edge cases. But what about code that uses other code? Do I fake ("mock") every external call to avoid depending on that code? Or do I actually want to test that my code makes those external calls correctly? What if the code is part of a class? I can't test instance methods without instantiating the class. But does that mean I'm breaking the unit testing philosophy because I'm implicitly testing the constructor too? What if an instance method modifies private state and doesn't produce any output to check? Should I test private methods at all? They are part of the nitty gritty implementation that could change without really affecting how the code performs. Do I really want tests that would have to be rewritten if I refactor the internal design?

And then there's integration testing. Most frequently I've seen integration testing introduced as the evil awful alternative to pure unit testing. Integration tests don't test your code in isolation but depend on external services like a database or an internet API. "Bad bad bad!" people say before explaining the "correct" approach&mdash;unit testing&mdash;with a few toy examples that don't translate to complex software. But surely we need some sort of integration test to prove our code really works, right? So are integration tests bad or just bad if we accidentally write one and call it a unit test?

To muddy the waters further, integration tests and unit tests are sometimes presented as the only types of tests. If it's not a unit test, then it's an integration test. However, it doesn't take much imagination to think up a situation that doesn't fit nicely into either category. As mentioned above, classes might make changes to private state that only manifest themselves when other methods are called. Another situation is "driver" code that calls a bunch of other code we've written to accomplish a bigger task. This doesn't seem suitable for a unit test since it isn't perfectly isolated and depends on other parts of the code working as intended. But it's also not a textbook integration test since it's not dependent on external services outside our control&mdash;all the code being run is still ours. If it breaks, it's still our responsibility to fix.

![Testing is confusing](/assets/20240127/testing.png){: style="max-width: 600px; display: block; margin: 30px auto;" }

And it keeps getting worse. We usually want to fully test our code so every line gets run (100% code coverage), but that requires special knowledge about what's going on inside the function itself. Instead of a black-box approach that checks if it does the right thing as far as the outside world is concerned, we now have to take a white-box approach designed to check the `if` statements and `while` conditions of the implementation itself.

Needless to say, this is all very confusing.

## Better testing knows the role of each test

We can do better. In my opinion, the real danger isn't writing unit vs. integration tests, dependencies between tests,  coupling to implementation, or any of that. It's writing one type of test and calling it something else. Or worse, writing one type of test and truly believing it's something else.

The truth is that we need all sorts of tests to build software confidently. They check for different things and do it in different ways, but they all provide evidence that our code is working well. The critical piece that ties all this together coherently is having a good taxonomy&mdash;organized mental model&mdash;that let's us understand the role of each test type. This is simpler than you might think and produces an incredibly informative suite of tests.

Here's how I break it down. Each test has two defining factors: 1) *what* is it trying to test and 2) *how* is it going to test that. Let's look first at *what* a test might written to check. I split the "what" into two types:

- An **interface** or **API** test. Interfaces and APIs are the "contracts" that specify what the code is expected to do without caring about how it works on the inside. For you system engineers out there, these are like stakeholder requirements.[^2] Somebody told you what the system should do without being picky about how you write the code to do that. These help us *validate* the software&mdash;prove that we solved the right problem by checking that the black box behaves like we expect.
- An **implementation** test. The implementation what the code does on the inside. This is where we might shoot for 100% code or branch coverage. It's coupled to our implementation since if we refactor the code we could end up with new branches to check even if the black-box behavior stays the same. The refactor would require updating implementation tests but not API-level tests. In systems engineering language, these tests might be like evaluating system requirements that stem from our specific solution to the stakeholder problem. These help us *verify* the software&mdash;prove to us that each piece inside our solution does the job we gave it. We still need API tests to make sure our designed solution matches the actual problem.

Next, I use three categories for "how" these tests might check for the right behavior:

- **Unit tests**. The "purest" tests that tell us if an isolated piece of code behaves correctly. We give it inputs and we check the outputs. I lean toward a looser definition of "units", "inputs", and "outputs" that better fits practical code. A unit could be a basic function with input arguments and output return values, but it could also be a class instance where checking the "output" is more about making sure the right calls are made to injected dependencies. If that's how we expect the unit to interact with the rest of the code, we should check it. We might create a class that publishes status notifications, so "checking the output" means ensuring the publisher is actually triggered.

  <aside><p>Note that how we define "unit" depends on whether this test is intended for APIs or implementations! An "API unit" might consist of multiple "implementation units." It's still an unit test from an API perspective, but additional tests focused on the internal details would fall to the implementation-level which might have its own tests, unit or otherwise.</p></aside>

- **Automated tests**. This is the category I use for those not-unit-tests-but-also-not-integration-tests. The tests are automated checks of how all our units work together. We might have driver code that calls a few units to perform a task, so an automated test ensures we put those pieces together correctly. For example, we might check that our robot's initialization code publishes the expected status messages. This involves a few other steps handled by smaller units, like setting up the publisher, building the message, and actually publishing it. All three could pass unit tests, but if the initialization code forgets the last step of actually publishing the message, we wouldn't know unless we have an automated test for the whole procedure. Distinguishing these tests from unit tests reminds us that a failure might be caused by a failing unit rather than a bug in the driving code itself.

- **Integration tests**. At *some* point, we need to make sure the whole system works in a realistic environment. Maybe our unit and automated tests pass, but the whole thing croaks in the real world because it doesn't run fast enough. Theoretically, our spacecraft should trigger rentry into the Earth's atmosphere at the planned time, but it also needs to send telemetry and check the tank pressures and respond to ground commands and suddenly woops we missed the deadline and we've got a date with a polar bear in the arctic instead of landing in the balmy carribbean. Or something like that.

Here are a few examples inspired by spacecraft flight software (NASA's [Core Flight System](https://github.com/nasa/cFS)):

| WHAT                 | HOW             | Example                                                      |
| -------------------- | --------------- | ------------------------------------------------------------ |
| ***API***            | **Integration** | With the full flight software running, send a command changing the mission state via a UDP socket and check that the command acknowledgement is published with the correct result within 1 sec of receiving the command. |
|                      | **Automated**   | Check that the flight manager processing loop reads commands from the message bus and creates and publishes an acknowlegement message with the result. |
|                      | **Unit**        | Check that the command handler ignores commands to change into disallowed mission states given the current state and a table of allowed transitions. |
| ***Implementation*** | **Integration** | Check that the Global Navigation Satellite System (GNSS) task can receive data from the real GNSS module via a serial link. |
|                      | **Automated**   | Check that every logical path through the flight manager's processing loop behaves as expected. |
|                      | **Unit**        | Check that a `should_allow_transition` function used by the flight manager's command processor returns the correct `true` or `false` value for all possible combinations of current state and requested state. |

## Organized tests improve problem solving

All this produces a delightfully clean interpretation of tests.

Running just the API tests checks our code against the big-picture stakeholder requirements. Our stakeholder or product owner can look at them and see that 1) we have defined the problem correctly (the test descriptions) and 2) we are solving that problem (the tests pass). They don't need to wade through implementation tests that are really for our (the developers') own benefit. For the spacecraft example we might see something like:[^3]

```test
[api.integration]
TestFlightManager::test_publishes_state_cmd_ack_within_deadline PASSED
[api.automated]
TestFlightManager::test_loop_reads_cmd_publishes_ack PASSED
[api.unit]
TestFlightManagerLib::test_handler_ignores_invalid_transition_cmd PASSED
```

In contrast, implementation tests tell us if our chosen internal design works as intended:

```test
[impl.integration]
TestGNSS::test_reads_module_serial_packet PASSED
[impl.automated]
TestFlightManager::test_loop_inc_failure_count_if_cmd_times_out PASSED
TestFlightManager::test_loop_updates_internal_state_if_cmd_succeeds PASSED
[impl.unit]
TestFlightManagerLib::test_should_allow_transition_matches_table PASSED
```

If our API tests end up failing, our implementation tests can help us determine whether we solved the wrong problem (implementation tests are all passing) or if we implemented our design incorrectly (implementation tests are failing). Take this example:

```test
[api.unit]
TestFlightManagerLib::test_handler_ignores_invalid_transition_cmd FAILED
[impl.unit]
TestFlightManagerLib::test_should_allow_transition_matches_table PASSED
```

Here we see that our implementation unit test is passing, but our design isn't properly utilizing the result to satisfy the API-level requirement. This tells us to check if we're using the right transition table and/or respecting the output of the `should_allow_transition` function when we check the command validity. Alternatively, we might see:

```test
[api.unit]
TestFlightManagerLib::test_handler_ignores_invalid_transition_cmd FAILED
[impl.unit]
TestFlightManagerLib::test_should_allow_transition_matches_table FAILED
```

While it's still possible the table isn't right or we aren't using the implementation function properly, the failing implementation test reveals that even if we *did* have those squared away, we have a bug in `should_allow_transition` that must be addressed before we can properly check the API-level behavior. 

<aside><p>Notice that the API test depends on the implementation test, but we're fine with it because we know for the API to work, <em>of course</em> it requires that the implementation also works. If we didn't make the API vs. implementation distinction, we  might see the API test as "bad" because of this dependency, but it's not. It just has a different purpose. Flagging it as we did captures the dependency, so instead of wasting time debugging the handler&mdash;which might work perfectly&mdash;the failing implementation test tells us we need to fix <code>should_allow_transition</code> first.</p></aside>

## Testing makes you write better code

As we've seen, testing helps us prove our code works as intended, but writing tests for real code can be daunting. Identifying tests as API/Implementation and Unit/Automated/Integrated is a framework that can guide us towards writing good tests. For example, we can tell that API unit tests should focus on single, logical black boxes while automated implementation tests have more leeway to ensure orchestration code performs the right design-specific steps. This produces better tests and, as a result, better software.

However, don't overlook another subtle benefit of testing: writing tests means we first have to write testable code, and testable code can often be better, cleaner, more efficient, and more organized. Testable code ends up like this for two reasons. First, if we sit down to write tests and realize we've so intertwined the bits of our software that there are no clearly discernible units *to* test, then we have a problem. Fix that and the code will be both more testable and more organized. Second, looking at our implementation to write a corresponding test lets us review and revise what we've written just like an author revises a draft. You might find duplicate lines, unecessary branching, missing exception handlers, etc. 
While it can feel burdensome, testing isn't pointless&mdash;writing good tests with the right taxonomy elevates our software's robustness *and* improves the architecture and design itself.

***NOTES***

[^1]: I'm not 100% advocating for this since a lot of the software architecture won't even exist yet so it's hard to really write a test, even a failing test, at this stage. Not to get ahead of myself and the rest of this post, but TDD only gives us API tests to work off of. We'll still have to write implementation tests.

[^2]: We could actually put requirement numbers in the test names if we wanted to make this connection explicit.

[^3]: In general, integration tests won't run in any CI/CD pipeline since they can't be run without external dependencies or need to run in "real" time (i.e., we don't mock system time calls). So even though you might not see the integration tests in a test report like this, I'm showing them like this to keep the example simple.

<script type="text/javascript">
  document.querySelectorAll("code.language-test").forEach(e => {
    // Find section headers
    e.innerHTML = e.innerHTML.replace(/\[.*\]/g, x => `<em style="color:#aaa;">${x}</em>`);
    e.innerHTML = e.innerHTML.replace(/PASSED/g, x => `<strong style="color:limegreen;">${x}</strong>`);
    e.innerHTML = e.innerHTML.replace(/FAILED/g, x => `<strong style="color:red;">${x}</strong>`);
    e.innerHTML = e.innerHTML.replace(/Test.*/g, x => `<span style="color:steelblue;">${x}</strong>`);
  });
</script>